<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8">
  <title>Hand Tracking AR Preview</title>
  <meta name="viewport" content="width=device-width,initial-scale=1.0">
  <!-- Google Model Viewer -->
  <script type="module" src="https://ajax.googleapis.com/ajax/libs/model-viewer/4.0.0/model-viewer.min.js"></script>
  <style>
    body { margin:0; background:#181818; font-family: 'Segoe UI',sans-serif; }
    #handinfo {
      position: absolute; top: 10px; left: 10px; z-index: 2;
      color: #fff; background:rgba(30,30,66,0.85); padding:12px; border-radius:8px;
      font-size: 1rem;
    }
    #input-video { width:0; height:0; position:absolute; }
    canvas.overlay {
      position: fixed; left:0; top:0; z-index:2; pointer-events: none;
      width: 100vw; height: 100vh;
    }
    #viewer {
      width: 100vw; height: 100vh;
      display: none; /* Shown only when hand detected */
      position: fixed; top:0; left:0; z-index:1;
      background: #181818;
    }
    #ar-button {
      position: absolute; bottom:40px; left: 50%; transform: translateX(-50%);
      display: none; /* Shown only when hand detected */
      z-index:8;
      font-size: 1.2rem;
      padding: 16px 32px;
      background: linear-gradient(135deg,#667eea,#764ba2); border:none;
      color:#fff; border-radius:8px; cursor:pointer; 
      box-shadow:0 2px 18px 0 rgba(0,0,0,0.35);
    }
  </style>
</head>
<body>
  <div id="handinfo">Waiting for hand...</div>
  <canvas class="overlay" id="overlay"></canvas>
  <video id="input-video" autoplay muted playsinline></video>
  
  <model-viewer
    id="viewer"
    src="./Diwali_crackers.glb"
    ios-src="./Diwali_crackers.usdz"
    ar
    ar-modes="webxr scene-viewer quick-look"
    ar-placement="floor"
    ar-scale="fixed"
    camera-controls
    tone-mapping="neutral"
    shadow-intensity="1"
    autoplay
    exposure="1"
    shadow-softness="0.5"
    style="display:none;"
  ></model-viewer>
  
  <button id="ar-button">View in AR</button>

  <script type="module">
    // MediaPipe Hand Tracking
    let HandLandmarker, FilesetResolver, handTracker;
    const info = document.getElementById('handinfo');
    const viewer = document.getElementById('viewer');
    const overlay = document.getElementById('overlay');
    const ctx = overlay.getContext('2d');
    let camW = 640, camH = 480;
    let handFound = false;

    async function setupMediaPipe() {
      const vision = await import('https://cdn.jsdelivr.net/npm/@mediapipe/tasks-vision@0.10.14/vision_bundle.js');
      HandLandmarker = vision.HandLandmarker;
      FilesetResolver = vision.FilesetResolver;
      const fileset = await FilesetResolver.forVisionTasks(
        'https://cdn.jsdelivr.net/npm/@mediapipe/tasks-vision@0.10.14/wasm'
      );
      handTracker = await HandLandmarker.createFromOptions(fileset, {
        baseOptions: { modelAssetPath: 'https://storage.googleapis.com/mediapipe-assets/hand_landmarker.task' },
        runningMode: 'VIDEO',
        numHands: 1
      });
    }

    async function startCamera() {
      const video = document.getElementById('input-video');
      const stream = await navigator.mediaDevices.getUserMedia({
        video: { facingMode: 'environment', width:camW, height:camH }, audio: false
      });
      video.srcObject = stream;
      await video.play();
      camW = video.videoWidth || camW;
      camH = video.videoHeight || camH;
      overlay.width = camW;
      overlay.height = camH;
      return video;
    }

    function drawPalm(pts) {
      ctx.clearRect(0, 0, camW, camH);
      if (!pts || pts.length < 21) return;
      ctx.fillStyle = "rgba(52,255,114,0.92)";
      pts.forEach(p => {
        ctx.beginPath();
        ctx.arc(p.x * camW, p.y * camH, 6, 0, Math.PI * 2);
        ctx.fill();
      });
      let px = (pts[0].x + pts[5].x + pts[17].x)/3;
      let py = (pts[0].y + pts[5].y + pts[17].y)/3;
      ctx.beginPath();
      ctx.arc(px*camW, py*camH, 10, 0, Math.PI*2);
      ctx.fillStyle = "rgba(255,222,55,0.96)";
      ctx.fill();
    }

    async function main() {
      await setupMediaPipe();
      const video = await startCamera();
      async function detectLoop() {
        const res = await handTracker.detectForVideo(video, video.currentTime);
        if (res.landmarks && res.landmarks.length) {
          const pts = res.landmarks[0];
          drawPalm(pts);
          let px = (pts[0].x + pts[5].x + pts[17].x) / 3;
          let py = (pts[0].y + pts[5].y + pts[17].y) / 3;
          info.textContent = "Hand detected. Tap 'View in AR'!";
          viewer.style.display = 'block';
          document.getElementById('ar-button').style.display = 'block';
          handFound = true;
        } else {
          info.textContent = "Waiting for hand...";
          viewer.style.display = 'none';
          document.getElementById('ar-button').style.display = 'none';
          handFound = false;
        }
        requestAnimationFrame(detectLoop);
      }
      detectLoop();

      // Launch model-viewer AR in response to user tap (browser required)
      document.getElementById('ar-button').addEventListener('click', () => {
        if (viewer.style.display !== 'none') {
          viewer.activateAR(); // Will open Quick Look, Scene Viewer, or WebXR, depending on device
        }
      });
    }

    main();
  </script>
</body>
</html>
