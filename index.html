<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8">
  <title>Hand Tracking AR with model-viewer</title>
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  <!-- Google model-viewer -->
  <script type="module" src="https://ajax.googleapis.com/ajax/libs/model-viewer/4.0.0/model-viewer.min.js"></script>
  <style>
    body { margin:0;padding:0;box-sizing:border-box;background:#181818; }
    #handinfo {
      color: white;
      position: absolute; top: 10px; left: 10px; z-index: 100; padding: 8px;
      background: rgba(40,40,80,0.8); border-radius: 8px;
      font-size: 1rem; font-family: 'Segoe UI', sans-serif;
    }
    #input-video {
      width: 0; height: 0; position: absolute;
    }
    #viewer {
      width: 100vw; height: 100vh;
      display: none; /* Start hidden until hand detected */
      position: fixed; top:0; left:0; z-index: 99;
      background: #181818;
    }
    canvas.overlay {
      position: fixed; left:0; top:0; z-index:100;
      pointer-events: none; width:100vw; height:100vh;
    }
  </style>
</head>
<body>
  <!-- Overlay for hand debug visuals -->
  <canvas class="overlay" id="overlay"></canvas>
  <!-- Diagnostic text -->
  <div id="handinfo">Waiting for hand...</div>
  <!-- MediaPipe camera -->
  <video id="input-video" autoplay muted playsinline></video>
  <!-- model-viewer only shown after hand detected -->
  <model-viewer
    id="viewer"
    src="./Diwali_crackers.glb"
    ios-src="./Diwali_crackers.usdz"
    ar
    ar-modes="webxr scene-viewer quick-look"
    ar-placement="floor"
    ar-scale="fixed"
    camera-controls
    tone-mapping="neutral"
    shadow-intensity="1"
    autoplay
    exposure="1"
    shadow-softness="0.5"
  >
    <button slot="ar-button" id="ar-button" class="ar-button">View in AR</button>
  </model-viewer>

  <!-- MediaPipe + AR placement logic -->
  <script type="module">
    // Import MediaPipe tasks loader
    let HandLandmarker, FilesetResolver, handTracker;
    const info = document.getElementById('handinfo');
    const viewer = document.getElementById('viewer');
    const overlay = document.getElementById('overlay');
    const ctx = overlay.getContext('2d');
    let camW = 640;
    let camH = 480;

    async function setupMediaPipe() {
      // Lazy load MediaPipe hand tracker
      const vision = await import('https://cdn.jsdelivr.net/npm/@mediapipe/tasks-vision@0.10.14/vision_bundle.js');
      HandLandmarker = vision.HandLandmarker;
      FilesetResolver = vision.FilesetResolver;
      const fileset = await FilesetResolver.forVisionTasks(
        'https://cdn.jsdelivr.net/npm/@mediapipe/tasks-vision@0.10.14/wasm'
      );
      handTracker = await HandLandmarker.createFromOptions(fileset, {
        baseOptions:{modelAssetPath:
          'https://storage.googleapis.com/mediapipe-assets/hand_landmarker.task'},
        runningMode:'VIDEO',
        numHands:1
      });
    }

    async function startCamera() {
      let video = document.getElementById('input-video');
      let stream = await navigator.mediaDevices.getUserMedia({
        video: { facingMode: 'environment', width: camW, height: camH }, audio:false });
      video.srcObject = stream;
      await video.play();
      camW = video.videoWidth || camW;
      camH = video.videoHeight || camH;
      overlay.width = camW;
      overlay.height = camH;
      return video;
    }

    function drawPalm(pts) {
      ctx.clearRect(0,0,camW,camH);
      if (!pts || pts.length < 21) return;
      // Draw all landmarks
      ctx.fillStyle = "rgba(50,255,100,0.9)";
      for (let p of pts) {
        ctx.beginPath();
        ctx.arc(p.x*camW, p.y*camH, 6, 0, Math.PI*2);
        ctx.fill();
      }
      // Draw palm center (average wrist/index/pinky base)
      let px = (pts[0].x + pts[5].x + pts[17].x)/3;
      let py = (pts[0].y + pts[5].y + pts[17].y)/3;
      ctx.beginPath();
      ctx.arc(px*camW, py*camH, 10, 0, Math.PI*2);
      ctx.fillStyle = "rgba(255,220,50,0.95)";
      ctx.fill();
    }

    function placeModelOnPalm(palmX, palmY, palmZ=0.5) {
      // For AR: set a hotspot or root transform so model aligns with palm
      // model-viewer API: move scene graph root if available (experimental), else use hotspots
      // Here, we'll just show the model-viewer for now
      // Advanced: Call viewer.updateHotspot({ name:'palm', position: `${palmX} ${palmY} ${palmZ}` })
      // Or update scene position if using the scene-graph API
    }

    async function main() {
      await setupMediaPipe();
      let video = await startCamera();
      let handFound = false;
      let lastPalmX=0, lastPalmY=0, lastPalmZ=0.5;

      // Main process loop
      async function detectLoop() {
        ctx.clearRect(0,0,camW,camH);
        let res = await handTracker.detectForVideo(video, video.currentTime);
        if (res.landmarks && res.landmarks.length) {
          let pts = res.landmarks[0];
          drawPalm(pts);
          let px = (pts[0].x + pts[5].x + pts[17].x)/3;
          let py = (pts[0].y + pts[5].y + pts[17].y)/3;
          info.textContent = "Hand detected â€” Tap 'View in AR'!";
          viewer.style.display = 'block';
          handFound = true;
          lastPalmX = px;
          lastPalmY = py;
          placeModelOnPalm(px, py);
        } else {
          info.textContent = "Waiting for hand...";
          viewer.style.display = 'none';
        }
        requestAnimationFrame(detectLoop);
      }
      detectLoop();
    }
    main();

    // Optional: Hook into AR start to apply palm placement when activating AR
    viewer.addEventListener('ar-status', e => {
      if (e.detail.status === 'session-started' && handFound) {
        // Place or update model based on latest palm average location
        placeModelOnPalm(lastPalmX, lastPalmY, lastPalmZ);
      }
    });
  </script>
</body>
</html>
